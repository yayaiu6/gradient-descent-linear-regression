Gradient Descent for Linear Regression
ğŸ“Œ Introduction
This project demonstrates how to implement Linear Regression using Gradient Descent to find the optimal parameters (w and b) for fitting a dataset.

ğŸ” Algorithm Overview
The goal is to determine the best parameters (w, b) that minimize the error between the actual values y and the predicted values Å· using the equation:

ğ‘¦
^
=
ğ‘¤
â‹…
ğ‘¥
+
ğ‘
y
^
â€‹
 =wâ‹…x+b
The error is measured using the Mean Squared Error (MSE) loss function:

Loss
=
1
ğ‘
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
^
)
2
Loss= 
N
1
â€‹
 âˆ‘(yâˆ’ 
y
^
â€‹
 ) 
2
 
where N is the number of data points.

ğŸ› ï¸ How It Works
1ï¸âƒ£ Data Initialization
Generate random dataset x and compute y using a linear equation with some noise.
Initialize parameters w = 0 and b = 0.
2ï¸âƒ£ Gradient Descent Optimization
Gradient Descent updates the values of w and b by computing the partial derivatives of the loss function:

âˆ‚
Loss
âˆ‚
ğ‘¤
=
âˆ’
2
ğ‘
âˆ‘
ğ‘¥
(
ğ‘¦
âˆ’
(
ğ‘¤
ğ‘¥
+
ğ‘
)
)
âˆ‚w
âˆ‚Loss
â€‹
 =âˆ’ 
N
2
â€‹
 âˆ‘x(yâˆ’(wx+b))
âˆ‚
Loss
âˆ‚
ğ‘
=
âˆ’
2
ğ‘
âˆ‘
(
ğ‘¦
âˆ’
(
ğ‘¤
ğ‘¥
+
ğ‘
)
)
âˆ‚b
âˆ‚Loss
â€‹
 =âˆ’ 
N
2
â€‹
 âˆ‘(yâˆ’(wx+b))
The parameters are updated using the following rules:

ğ‘¤
=
ğ‘¤
âˆ’
ğ›¼
â‹…
âˆ‚
Loss
âˆ‚
ğ‘¤
w=wâˆ’Î±â‹… 
âˆ‚w
âˆ‚Loss
â€‹
 
ğ‘
=
ğ‘
âˆ’
ğ›¼
â‹…
âˆ‚
Loss
âˆ‚
ğ‘
b=bâˆ’Î±â‹… 
âˆ‚b
âˆ‚Loss
â€‹
 
where Î± (alpha) is the learning rate.

ğŸ’» Code Implementation
This algorithm is implemented in Python using NumPy:

python
Ù†Ø³Ø®
ØªØ­Ø±ÙŠØ±
import numpy as np

# Generate random dataset
x = np.random.randn(10,1)
y = 2 * x + np.random.randn()

# Initialize parameters
w, b = 0.0, 0.0
learning_rate = 0.01

# Gradient Descent function
def descent(x, y, w, b, learning_rate):
    dldw, dldb = 0.0, 0.0
    N = x.shape[0]
    for xi, yi in zip(x, y):
        dldw += -2 * xi * (yi - (w * xi + b))
        dldb += -2 * (yi - (w * xi + b))
    w -= learning_rate * (1/N) * dldw
    b -= learning_rate * (1/N) * dldb
    return w, b

# Training loop
for epoch in range(1050):
    w, b = descent(x, y, w, b, learning_rate)
    yhat = w * x + b
    loss = np.sum((y - yhat) ** 2) / x.shape[0]
    print(f'Epoch {epoch}: Loss = {loss:.6f}, w = {w[0]}, b = {b}')
ğŸš€ Key Features
âœ… Implements Gradient Descent from scratch
âœ… Uses NumPy for efficient numerical operations
âœ… Demonstrates Linear Regression without relying on external ML libraries

ğŸ“¢ How to Use
Clone the repository:
sh
Ù†Ø³Ø®
ØªØ­Ø±ÙŠØ±
git clone https://github.com/YOUR_USERNAME/gradient-descent-linear-regression.git
cd gradient-descent-linear-regression
Run the script:
sh
Ù†Ø³Ø®
ØªØ­Ø±ÙŠØ±
python gradient_descent.py
ğŸ“š References
Gradient Descent - Wikipedia
Linear Regression - Stanford CS229
